\chapter{Formalism}
\section{Hilbert Space}

The purpose of this chapter is to recast the theory in a more powerful form.

Quantum theory is based on two constructs: \textit{wave functions} and \textit{operators}. The state of a system is represented by its wave function, obervables are represented by operators. Mathematically, weave functions satify the defining conditions for abstract \textbf{vectors}, and operators act on them as \textbf{linear transformations}. So the natural language of quantum mechanics is \textbf{linear algebra}.

But it is not, I suspect, a form of linear algebra with which you are immediately familiar. In an $N$-dimensional space it is simplest to represent a vector $\ket{\alpha}$, by the $N$-tuple of its components, $\{a_n\}$, with respect to a specified orthonormal basis:
\begin{equation}\label{3.1}
	\ket{\alpha}\to\vb*{a}=\mqty(a_1\\a_2\\\vdots \\a_N)
\end{equation}
The \textbf{inner product}, $\braket{\alpha}{\beta}$, of two vectors (generalizng the dot product in three dimensions) is the complex numebr,
\begin{equation}\label{3.2}
	\braket{\alpha}{\beta}=a_1^*b_1+a_2^*b_2+\cdots +a_N^*b_N
\end{equation}
Linear transformations, $T$, are represented by \textbf{matrices} (with respect to the specified basis), which act on vectors (to produce new vectors) by the ordinary rules of matrix multiplication:
\begin{equation}\label{3.3}
	\ket{\beta}=T\ket{\alpha}\to \vb{b}=\vb{T}\vb{a}=\mqty(t_{11}&t_{12}&\cdots & t_{1N}\\
	t_{21}&t_{22}&\cdots&t_{2N}\\
	\vdots&\vdots&\vdots&\vdots\\
	t_{N1}&t_{N2}& \cdots &t_{NN})
\end{equation}
But the "vectors" we encounter in quantum mechanics are (for the most specified) \textit{functions}, and they live in infinite-dimensional spaces. For them the $N$-tuple/matrix notation is awkward, at best, and manipulations that are well-behaved in the finite dimensional case can be problematic. (The underlying reason is that whereas the finite sum in (\ref{3.2}) always exists, an infinite sum--or a integral-- may not converge, in which case the inner product does not exists, and any argument involving inner products is immediately suspect.).

The collection of all functions of $x$ constitutes a vector space, but fot our purposes it is much too latge. To represent a possible physical state, the function $\Psi$ must be normalized:
$$\int |\Psi|^2\dd x=1$$ The set of all \textbf{square-integrable functions}, on a specified ineterval\footnote{Fot is, the limits ($a$ and $b$) will almost always be $\\pm\infty$, but we might as well keep thing more general for the moment},
\begin{equation}\label{3.4}
	f(x)\qquad \mbox{ such that  }\qquad \int_a^b |f(x)|^2\dd x<\infty
\end{equation}
constitutes a (much smaller) vector space. Mathematicians call it $L_2(a,b)$; physicist call it \textbf{Hilbert space}. In quantum mechanics then
\begin{equation}\label{3.5}
	\boxed{\mbox{\textbf{Wave functions live in Hilbert space.}}}
\end{equation}
We define the \textbf{inner product of two functions}, $f(x)$ and $g(x)$, as follows:
\begin{equation}\label{3.6}
	\braket{f}{g}\equiv\int_a^b f(x)^*g(x)\dd x
\end{equation}
If $f$ and $g$ are both square-integrable (that is, if they are both in hilber spaces), their inner product is guaranteed to exist. This follows from the integral \textbf{Schwartz inequality}:
\begin{equation}\label{3.7}
	\left|\int_a^bf(x)^*g(x)\dd x\right|\leq \sqrt{\int_a^b|f(x)|^2\dd x\int_a^b|g(x)|^2\dd x}
\end{equation}
Notice in particular that
\begin{equation}\label{3.8}
	\braket{g}{f}=\braket{f}{g}^*
\end{equation}
Moreover, the inner product of $f(x)$ with itself,
\begin{equation}\label{3.9}
	\braket{f}{f}=\int_a^b|f(x)|^3\dd x
\end{equation}
is real and and non-negative; it's zero only when $f(x)=0$. 

A function is said to be \textbf{normalized} if its inner product with itself is $1$; two functions are \textbf{orthogonal} of their inner product is $0$; and a set of functions, $\{f_n\}$, is \textbf{orthonormal} of they are normalized and mutually orthogonal
\begin{equation}\label{3.10}
	\braket{f_m}{f_n}=\delta_{mn}
\end{equation}
Finally, a set of functions is \textbf{compelte} if any other function (in Hilbert space) can be expreded as a linear combination of them:
\begin{equation}\label{3.11}
	f(x)=\sum_{n=1}^\infty c_nf_n(x)
\end{equation}
If rge functuons $\{f_n(x)\}$ are orthonrmal, the coefficients are given by Fourier's trick
\begin{equation}\label{3.12}
	c_n=\braket{f_n}{f}
\end{equation}

\section{Obervables}
\subsection{Hermitian Operators}
The expectation value of an observable $Q(x,p)$ can be expressed very neatly in inner-product notation:
\begin{equation}\label{3.13}
	<Q>=\int \Psi^*\hat{Q}\Psi\dd x=\bra{\Psi}\ket{\hat{Q}\Psi}
\end{equation}
Now, the outcome of a measurement has got to be \textit{real}, and so, a fortiori, is the average of many measurements:
\begin{equation}\label{3.14}
	<Q>=<Q>^*
\end{equation}
But the complex conjugate of an inner product reverses the order (\ref{3.8}), so
\begin{equation}\label{3.15}
	\bra{\Psi}\ket{\hat{Q}\Psi}=\bra{\hat{Q}\Psi}{\ket\Psi}
\end{equation}
and this must hold true for any wave function $\Psi$. Thus operators representing \textit{obervables} have the very special property that
\begin{equation}\label{3.16}
	\bra{f}\ket{\hat{Q}f}=\bra{\hat{Q}f}\ket{f}\qquad\mbox{for all $f(x)$}
\end{equation}
We call such operators \textbf{hermitian}.

The essentiak point is that a hermitian operator can be applied either to the first member of an inner product or to the second, with the same result, and hermitian operators naturally arise in quantum mechanics because their expectations values are real:
\begin{equation}\label{3.18}
	\boxed{\mbox{\textbf{Obervables are represented by hermitian operators.}}}
\end{equation}
Is the momentum operator, for example, hermitian?
\begin{equation}\label{3.19}
	\bra{f}\ket{\hat{p}g}=\int_{-\infty}^\infty \frac{\hbar}{i}\dv{g}{x}\dd x=\frac{\hbar}{i}\eval{f^*g}_{-\infty}^\infty +\int_{-\infty}^\infty \left(\frac{\hbar}{i}\dv{f}{x}\right)^*g\dd x=\bra{\hat{p}f}\ket{g}
\end{equation}
Notice if $f(x)$ and $g(x)$ are squared integrable, they must go to zero at $\pm\infty$.

\subsection{Determinate States}
Ordinarily, when you measure an obervable $Q$ on an ensamble of identically prepared system, all in the same state $\Psi$, you do not ger the same reuslt each time--this is the indeterminancy of quantum mechanics. \textit{Question:} Would it be possible to prepare a state such that \textit{every} measurement of $Q$ is certain to return the \textit{same} value (call it $a$)? This would be, if younlike, a \textbf{determinate state}, for the obervable $Q$ (Actually, we already know one example: Stationary states are dterminate states of Hamiltonian; a measurement of the total enery, on a particle in the stationary state $\Psi_n$, is certain to yield the corresponding "allowed" energy $E_n$).

Well, the standard deviation of $Q$, in a determiante state, would to be zero, shich is to say,
\begin{equation}\label{3.21}
	\sigma^2=<(\hat{Q}-<Q>)^2>=\bra{\Psi}\ket{(\hat{Q}-q)^2\Psi}=\bra{(\hat{Q}-q)\Psi}\ket{(\hat{Q}-q)\Psi}=0
\end{equation}
But the only function whose inner product with itself vanishes is $0$, so
\begin{equation}\label{3.22}
	\hat{Q}\Psi=q\Psi
\end{equation}
This is the \textbf{eigenvalue equation} for the operator $\hat{Q}$; $\Psi$ is an \textbf{eigenfunction} of $\hat{Q}$, and $q$ is the corresponding \textbf{eigenvalue}. Thus
\begin{equation}\label{3.23}
	\boxed{\mbox{\textbf{Determiante states are eigenfunctions of }$\hat{Q}$}}
\end{equation}
Measurements of $Q$ aÂ¡on such states is certain to yield the eigenvalue, $q$.

Note that the eigenvalue is a number (not a operator or a function). You can multuply any eigenfunction by a constant,  and it is still an eigenfunction, with the same eigenvalue. Zero does not count as an eigenfunction. But there's nothing wrong with zero as an eigenvalue. The collection of all the eigenvalues of an operator is called its \textbf{specturm} . Sometimes two (or more) linearly independent eigenfunctions share the same eigenvalue; in that case the spectrum is said to be \textbf{degenerate}.

\section{Eigenfunctions of a Hermitian Operator}
Our attention is thus directed to the \textit{eigenfunction of hermitian operators} (physically: determine states of obervables). These fall into two categories: If the spectrum is \textbf{discrete} (i.e., the eigenvalues are separated from one another) then the eigenfunctions lie in Hilbert space and they constitute physically realizable states. If the spectrum is \textbf{contonuous} (i.e., the eigenvalues fill out an entire range) then the eigenfunctions are non normalizable, and they do not represent possible wave functions (\textit{though linear cominations} of them--involving neccessarily a spread in eigenvalues--may be normalizable). Some operator have a discrete spectrum only (for example, the Hamiltonian for the harmonic oscillator). some have only a continuous spectrum (for example, the free partivle Hamiltonian), and some have both discrete part and a continuous part (for example, the Hamiltonian for a finite square well). The discrete case is easier to handle. because the relevant inner product are guaranteed to exist--in fact, it is very similar to the finite-dimensional theory (the eigenvectors of a hermitian matrix). I'll treat the discrete case first, and then the continuous one:

\subsection{Discrete Spectra}
Mathematically, the normalizable eigenfunctions of a hermitian have two important properties:
\begin{teo}
	Their \textit{eqigenvalues} are \textit{real}.
\end{teo}
This is comforting: If you measure an obervable on a particle ina  dterminate state, you will at leat least get a real number.
\begin{teo}
	Eigenfunctions belonging to distinct eigenvalues are \textit{orthogonal}.
\end{teo}
In  a \textit{finite}-diemsnional vector space the eigenvectos of a hermitian matrix have a third fundamental property: The span the space (every vector can be expressed as a linar combination of them). Unfortunately, the proof does not generalize to infinite-dimensional spaces. But the property itslef is essential to the internal consistency of quantum mechanics, so we will take it as an \textit{axiom} (or, more precisely, as a restriction on the class of hermitian operators that can represent obervables):
\begin{axiom}
	The eigenvalues of an obervable operator are \textit{compelte}: Any function (in Hilber space) can be expressed as a linear combination of them.
\end{axiom}

\subsection{Continuous Spectra}
If the spectrum of a hermitian operator is \textit{continuous}, the eigenfunctions are not normalizable, and the proofs of Theorems 1 and 2 fail, because the inner products may not exists. Nevertheless, there is a sense in which the three essential properties (reality, orthogonality, and complereness) still hold. I think it's best to approach this subtle case through specific examples.

\begin{example}
	Find the eigenfunctions and eigenvalues of the momentum operator
\end{example}
\begin{sol}
\end{sol}
Let $f_p(x)$ be the eigenfunction and $p$ the eigenvalue:
\begin{equation}\label{3.30}
	\frac{\hbar}{i}\frac{\dd}{\dd x}f_p(x)=pf_p(x)
\end{equation}
The general solution is $$f_p(x)=Ae^{ipc/\hbar}$$ This is not square-integrable, for any (complex) value of $p$--the momentum operator has no eigenfunctions in Hilbert space. And yet, if we restrict ourselves to \textit{real} eigenvalues. we do recover a kind of \textit{ersatz} "orthonormality". ...

\subsection{Generalized Statistical Interpretation}
\textbf{Generalized statistical interpretation:} If you measure an obervable $Q(x,p)$ on a particle in the state $\Psi(x,t)$, you are certain to get \textit{one of the eigenvalues} of the hermitian operator $\hat{q}(x,-i\hbar\dd/\dd x)$. If the spectrum of $\hat{Q}$ is discrete, the probability of getting the particular eigenvalue $q_n$ associated with the orthonormalized eifengunction $f_n(x)$ is
\begin{equation}\label{3.43}
	|c_n|^2,\qquad \mbox{where }\qquad c_n=\braket{f_n}{\Psi}
\end{equation}
If the sprectrum is continuous, with real eigenvalues $q(z)$ and associated Dirac-orthonormalized eigenfunctions $f_z(x)$, the probability of gettin a result in the range $\dd z$ is
\begin{equation}\label{3.44}
	|c(z)|^2\dd z \mbox{where }\qquad c(z)=\braket{f_z}{\Psi}
\end{equation}
Upon measurement, the wave function "collapses" to the corresponding eigenstate.

The statistical interpretation is raadically different from anything we encounter in classical physics. A somewhat different persperctive helps to make it plausible:
The eigenfunctions of an observable operator are \textit{complete}, so the wave function can be written as a linear combination of them:
\begin{equation}\label{3.45}
	\Psi(x,t)=\sum_nc_nf_x(x)
\end{equation}
(For simplicity, I'll assume that the spectrum is discrete; it's easy to generalise this argument to the continuous case.) Because the eigenfucntions are \textit{orthonormal}, the coefficients are given by Fourier's trick:
\begin{equation}\label{3.46}
	c_n=\braket{f_n}{\Psi}=\int f_n(x)^*\Psi(x,t)\dd x
\end{equation}
Qualitatively, $c_n$ tells you "how much $f_n$ is contained in $\Psi$", and given that a mesuremnet has to return one of the eigenvalues of $\hat{Q}$, it seems reasonable that the probability of getting the particular eigenvalue $q_n$ would be determined by the "amount of $f_n$" in $\Psi$. But because probabilities are determined by the absolute square of the wave function, the precise measure is actually $|c_n|^2$. That's the essential burden of the generalized statistical interpretation.

Of course, the total probability (summed over all outcomes) has got to be one:
\begin{equation}\label{3.47}
	\sum_n|c_n|^2=1
\end{equation}
Similarly, the expectation value of $Q$ should be the sum over all possibles outcomes of the eigenvalue times the probability of getting the eigenvalue:
\begin{equation}\label{3.49}
	<Q>=\sum_nq_n|c_n1^2
\end{equation}
The \textbf{momentum space wave function}, $\Phi(p,t)$. It is essentially the \textit{Fourier transform} of the (\textbf{position space}) wave function $\Psi(x,t)$--which, by Plancherel'stheorem, is its \textit{inverse} Fourier transform:
\begin{equation}\label{3.54}
\Phi(p,t)=\frac{1}{\sqrt{2\pi\hbar}}\int_{-\infty}^\infty e^{-ipx/\hbar}\Psi(x,t)\dd x
\end{equation}
\begin{equation}\label{3.55}
	\Psi(x,t)=\frac{1}{\sqrt{2\pi\hbar}}\int_{-\infty}^\infty e^{ipx/\hbar}\Phi(p,t)\dd p
\end{equation}
According to the generalized statistical interpretationm, the probability that a measurement of momentum would yiled a result in the range $\dd p$ is
\begin{equation}\label{3.56}
	|\Phi(p,t)|^2\dd p
\end{equation}

\section{The Uncertaintly Principle}

\section{Dirac Notation}
Imagine an ordinary vactor $\vb{A}$ in two dimensions. How would you describe this vector to someone? The most convenient way is to set up catesian axes, $x$ and $y$, and specify the components of $\vb{A}$: $a_x=\hat{i}\cdot \vb{A}, A_y=\hat{j}\cdot \vb{A}$. Of course, your sister might have drawn a different set of axed, $x'$ and $y'$, and she would report differen components: $A'_x=\hat{i}'\cdot\vb{A}, A'_y=\hat{j}'\cdot\vb{A}$. But it's all the same vector--we're simply expressing it with respecto to two different bases ($\{\hat{i},\hat{j}\}$ and $\{\hat{i}',\hat{j}'\})$. The vector itself lives "out there is space", independent of anybody's (arbitraty) choice of coordinates.

The same is true for the state of a system in quantum mechancis. It is represented by a vector, $\ket{s(t)}$, that lives "out there in Hilbert space", but we can express it with respceto to any number of different bases. The wave function $\Psi(x,t)$ is actually the coefficient in the expansion of $\ket{s(t)}$ in the basis of position eigenfunctions:
\begin{equation}\label{3.75}
	\Psi(x,t)=\braket{x}{s(t)}
\end{equation}
(with $\ket{x}$ standing fot the eigenfucntions of $\hat{x}$ with eigenvalue $x$), whereas the momentum sapce wavefunction $\Phi(x,t)$ is the expansion of $\ket{s}$ in the absis of momentum eigenfucntions:
\begin{equation}\label{3.76}
	\Phi(p,t)=\braket{p}{s(t)}
\end{equation}
(with $\ket{p}$ standing for the eigenfunction of $\hat{p}$ with eigenvalue $p$). Or we could expand $\ket{s}$ in the basis of energy eigenfunctions (supposing for simplicity that the spectrum is discrete):
\begin{equation}\label{3.77}
	c_n(t)=\braket{n}{s(t)}
\end{equation}
(with $\ket{n}$ standing for thr $n$th eigenfunction of $\hat{H}$)--(\ref{3.46}). But it's all the same state; the functions $\Psi$ and $\Phi$, and the collection of coefficients $\{c_n\}$, contain exactly the same information--they are simply three different ways of describing the same vector:
\begin{align}
	\nonumber \Psi(x,t)&=\int\Psi(y,t)\delta(x-y)\dd y=\int\Phi(p,t)\frac{1}{\sqrt{2\pi\hbar}}e^{ipx/\hbar}\dd  p\\
										 &=\sum c_ne^{-iE_nt/\hbar}\psi_n(x)\label{3.78}
\end{align}
Operators (representing obervables) are linear transformations--they "transform" one vector into another:
\begin{equation}\label{3.79}
	\ket{\beta}=\hat{Q}\ket{\alpha}
\end{equation}
Just as vectors are represented, with respect to a particular basis $\{\ket{e_n}\}$\footnote{I'll assume the basis is discrete: otherwise $n$ becomes a continuous index and the syms are replaced by integrals}, by their components,
\begin{equation}\label{3.80}
	\ket{\alpha}=\sum_n a_n\ket{e_n}, \mbox{with } a_n=\braket{e_n}{\alpha}:\quad \beta=\sum_n b_n\ket{e_n}, \mbox{with } b_n=\braket{e_n}{\beta}
\end{equation}
operators are represented (with respect to a particular basis) by their \textbf{matrix elements}\footnote{Tjis terminology is inspired, obviously, by the finite-dimensinal case, but the "matrix" will now typically have an infinite (maybe even uncountable) number of elements.}
\begin{equation}\label{3.81}
	\mel{e_m}{\hat{Q}}{e_n}\equiv Q_{mn}
\end{equation}
In this notations (\ref{3.79}) takes the form
\begin{equation}\label{3.82}
	\sum_nb_n\ket{e_n}=\sum_na_n\hat{Q}\ket{e_n}
\end{equation}
or, taking the inner product with $\ket{e_m}$,
\begin{equation}\label{3.83}
	\sum_n b_n\braket{e_m}{e_n}=\sum_na_n\mel{e_m}{\hat{Q}}{e_n}
\end{equation}
and hence
\begin{equation}\label{3.84}
	b_m=\sum_nQ_{mn}a_n
\end{equation}
Thus the matrix elements tell you how the components transform. 

Later on we will encounter sysrem that admit only a finite number ($N$) of linearly independet states. In that case $\ket{s(t)}$ lives un an $N$-dimensional vector space; it can be represented as a column of ($N$) components (with respecto to a given bases), and operators take the form of ordinary ($N\times N$) matrices. These are the simplest quantum systems--none of the subtleties associted with infinite-dimensional vector spaces arise. 

Dirac proposed to chop the bracket notation for the inner product, $\braket{\alpha}{\beta}$, into two pieces, which he called \textbf{bra}, $\bra{\alpha}$, and \textbf{ket} $\ket{\beta}$. The latter is a vector, but what exactly is the former? It's a \textit{linear function} of vectors, in the sense that when it hits a vector (to its right) it yields a (complex) number--the inner product. (When an \textit{operator} hits a vector, it delivers another vector; when a \textit{bra} hits a vector, it delivers a number.) In a function space, the bra can be thought of as an instruction to integrate: $$\bra{f}=\int f^*[\cdots]\dd x$$ with the ellipsis $[\cdot]$ waiting to be filled by whatever function the bra encounters in the jet to its right. In a finite-dimensional vector space, with the vectors expressed as columns,
\begin{equation}\label{3.87}
	\ket{\alpha}=\mqty(a_1\\a_2\\\vdots\\ a_n)
\end{equation}
the corresponding bra is a row vector:
\begin{equation}\label{3.88}
	\bra{\alpha}=\mqty(a_1^* a_2^*\cdots a_n^*)
\end{equation}
The collection of all bras constitutes another vector space--the so-called \textbf{dual space.}

The license to treat bras as separate entities in their own right allows for some powerful and pretty notation. For example, if $\ket{\alpha}$ is a normalized vector, the operator
\begin{equation}\label{3.89}
	\hat{P}\equiv \op{a}{a} 
\end{equation}
picks out the portion of any vector that "lies along" $\ket{\alpha}$: $$\hat{P}\ket{\beta}=\braket{\alpha}{\beta}\ket{\alpha}$$ we call it the \textbf{projection operator} onto the one-dimensional subspace spanned by $\ket{\alpha}$. If $\{\ket{e_n}\}$ is a discrete orthonormal basis,
\begin{equation}\label{3.90}
	\braket{e_m}{e_n}=\delta_{mn}
\end{equation}
then 
\begin{equation}\label{3.91}
	\sum_n\op{e_n}{e_n}=1
\end{equation}
(the identity operator). For if we les this operator act on any vector $\ket{\alpha}$, we recover the expansion of $\ket{\alpha}$ in the $\{\ket{e_n}\}$ basis:
\begin{equation}\label{3.92}
	\sum_n\ket{e_n}\braket{e_n}{\alpha}=\ket{\alpha}
\end{equation}
Similarly, if $\{\ket{e_z}\}$ is a \textit{Dirac} orthonormalized continuous basis,
\begin{equation}\label{3.93}
	\braket{e_a}{e_{z'}}=\delta(z-z')
\end{equation}
then
\begin{equation}\label{3.94}
	\int\op{e_z}{e_z}\dd z=1
\end{equation}
(\ref{3.91}) and (\ref{3.94}) are the tidiest ways ti express completeness.



































