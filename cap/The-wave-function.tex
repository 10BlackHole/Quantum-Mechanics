\chapter{The Wave Function}
\section{The Schrodinger Equation}
Imagine a particle of mass $m$, constrained to move along the $x$-axis, subjet to some to specifies force $F(x,t)$. The program of \textit{classical} mechanics is to determine the position of the particle at any give time: $x(t)$. Once we know that, we can figure out the velocity ($v=\dv*{x}{t}$), the momentum ($p=mv$), the kinetic energy ($T=(1/2)mv^2$), or any other dynamical variable of interest. And how do we go about determining $x(t)$? We apply Newton's second law: $F=ma$. (For \textit{conservative} systems--the only kins we shall consider, and, fortunately, the only kins that \textit{occur} at the microscopic level--the force can be expresed as the derivative of a potential energy function \footnote{Magnetic forces are an exception, but let's not worry about them just yet. By the eay, we shall assume for the moment that the motion is nonrelativistic ($v\ll c$)}, $F=-\pdv*{V}{x}$, and Newton's law reads $m\dd ^2x/\dd t^2=-\partial V/\partial x$) This, together with appropiate initial conditions (typically the position and velocity at $t=0$), determines $x(t)$ .

Quantum mechanics approaches this same problem quite differently. In this case what we're loocking for ir the particle's \textbf{wave function}, $\Psi(x,t)$, and we get it by solving the \textbf{Schrodinger equaation}:
\begin{equation}\label{1.1}\marginnote{Schrodinger equation}
	\boxed{i\hbar\pdv{\Psi}{t}=-\frac{\hbar^2}{2m}\pdv[2]{\Psi}{x}+V\Psi}
\end{equation}
This equation plays a role logically analogous to Newton's second law: Given suitable initial conditions (typically, $\Psi(x,0)$), the Schrodignes equation determines $\Psi(x,t)$ for all future time, just as, in classical mechanics, Newton's law determines $x(t)$ for all future time.

\section{The Statistical Interpretation}
What exactly is this "wave function"? Born's \textbf{statistical interpretation} of the wave function, says that $|\Psi(x,t)|^2 $ gives the \textbf{probability} of finding the particle at point $x$, at time $t$, or more precisely
\begin{equation}\label{1.3}\marginnote{Statistical interpretation}
	\boxed{\int_a^b|\Psi(x,t)|^2\dd x=\mbox{probability of finding the particle  between $a$ and $b$, at time $b$}}
\end{equation}
Probability is the \textit{area} inder the graph of $|\Psi(x,t)|^2$

Yhe statistical interpretation introduces a kind of \textbf{indeterminacy} into quantum mechanics, for even if you know evertything the theory has to tell you about the particle /to wit: its wave function, still you cannot predict with certainty the outcome of a simple experiment to measure its position-- all quantum mechanics has to offer is \textit{statistical} information about the \textit{possible} results.

We say that the wave function \textbf{collapses}, upon measurement, to spike some point. There are, then, two entirely distinct kinds of physical processes: "ordinary" oenes, in which the wave function evolves in a leisurely fashion inder the Schrodinger equation, and "measurements", in which $\Psi$ suddenly and discontinously collapses \footnote{The role of measurement in quantum mechanics is so critical and so bizarre that you may well be wondering what precisely constitutes a measurement. Does it have to do with the interaction between a miscroscopic (quantum) system and a mecorscopic (classical) measuring apparatus (as Bohr insisted), or is it intervention of a conscious "oberver" (as Wigner proposed)? For the moment let's take the naive view: A measurement is the kind of thing that a scientist does in the laboratory, with rules, stopwatches, and so on}.

\section{Probability}
I will give some definitions. The total number is  
\begin{equation}\label{1.4}
	N = \sum_{j=0}^\infty N(j)
\end{equation}
The probability of getting $j$ is
\begin{equation}\label{1.5}
	P(j)=\frac{N(j)}{N}
\end{equation}
In particular, the sum of all the probabilities is 1:
\begin{equation}\label{1.6}
	\sum_{j=0}^\infty P(j)=0
\end{equation}


In general, the average value of some \textit{function} of $j$ es given by
\begin{equation}\label{1.9}\marginnote{Average value of a function}
	\boxed{<f(j)>=\sum_{j=0}^\infty f(j)P(j)}
\end{equation}
In quantum mechanics the average is usually the quantity of interest; in that context it has come to be called the \textbf{expectation value}.

Now we need a numerial measure of the amount of "spread" in a distribution, with respect to the aveerge. The most obvious way to do this would be to find out how far each individual deviates from de average,
\begin{equation}\label{1.10}
	\Delta j=j-<j>
\end{equation}
and compute the average of $\Delta j$. Trouble is, of course, that you get zero, by the nature of the avergae, $\Delta j$ is as often negative as positive. To avoid this irritating problem you might decide to average de \textit{absolute value} of $\Delta j$. But absolute values are nasty to work with; instead, we get around the sign problem by \textit{squaring} before averaging:

\begin{equation}\label{1.11}\marginnote{Variance}
	\sigma^2\equiv <(\Delta j)^2>
\end{equation}
This quantity is known as the \textbf{variance} of the distribution; $\sigma$ itsel is called the \textbf{standard deviation}. 

There is a useful little theorem pn variances:
\begin{equation}\label{1.12}
	\sigma=\sqrt{<j^2> - <j>^2}
\end{equation}

\section{Continous Variables}
I hace assumed that we are dealing with a \textit{discrete} variable--that is, one that can take on only isolated values. But it is simple enough to generaliza to \textit{continous} distributions. If the interval is sufficiently short, this probabilty is \textit{proportional to the length of the interval}. This
\begin{equation}\label{1.14}\marginnote{Probabilty density}
	\mbox{{probabilty that an individual lies between $x$ and $(x+\dd x)$}}=\rho(x)\dd x
\end{equation}
The proportionality factor $\rho(x)$,is often loosely called "the probability of gettinf $x$", but is sloopy lanuage; a better term is \textbf{probability density}. The probability that $x$ lies between $a$ and $b$ (a finite interval) is gicen by the integral of $\rho(x)$:
\begin{equation}\label{1.15}
	P_{ab}=\int_a^b\rho(x)\dd x
\end{equation}
and the rules we deduced for discrete distributions translate in the obvious way:
\begin{equation}\label{1.16}
	1 = \int_{-\infty}^{+\infty}\rho(x)\dd x
\end{equation}
\begin{equation}\label{1.17}
	<x>=\int_{-\infty}^{+\infty}x\rho(x)\dd x
\end{equation}
\begin{equation}\label{1.18}
	<f(x)>=\int_{-\infty}^{+\infty}f(x)\rho(x)\dd x
\end{equation}
\begin{equation}\label{1.19}
	\sigma^2\equiv <(\Delta x)^2>=<x^2>-<x>^2
\end{equation}

\section{Normalization}
We return now to the statistical interpretation of the wave function (\ref{1.3}), which says that $|\Psi(x,t)|^2$ is the probability density for finding the particle at point $x$, at time $t$. It follows (\ref{1.6}) that the integral of $|\Psi|^2$ must be $1$ (the particle's got to be \textit{somewhere}):
\begin{equation}\label{1.20}
	\boxed{\int_{-\infty}^{+\infty}|\Psi(x,t)|^2\dd x=1}
\end{equation}

If $\Psi(x,t)$ is a solution of the Schrodinger equation , so too is $A\Psi(x,t)$, where $A$ is any (complex) constant. What we must do, then, is pick this undetermined multiplicative factor so as to ensure that (\ref{1.20}) is satisfied. This process is called \textbf{normalizing} the wave function. For some solutions to the Schrodinger equation the integral is \textit{infinite}; in that case \textit{no} multiplicative factor is going to make it $1$. The same goes for the trivial solution $\Psi=0$ . Shuch \textbf{non-normalizable} solutions cannot represent particles, and must be rejected. Physically realizable states correspond to the \textbf{square-integrable} solutions to Schrodinger equation.\footnote{Evidently $\Psi(x,t)$ must go to zero faster than $1/\sqrt{|x|}$, as $|x|\to\infty$. Incidentaly, normalziation only fixed the \textit{modulus} of $A$: the phase remains undetermiend. However, as we shall, the latter carries no physical significance anyway.}

Fortunately, the Schrodinger equation has the rematkable property that it automatically preserves the normalization of the wave function.

\section{Momentum}
For a particle in state $\Psi$, the expectation value of $x$ is 
\begin{equation}\label{1.28}\marginnote{Expectation value of $x$}
	\boxed{<x>=\int_{-\infty}^{+\infty}x|\Psi(x,t)|^2\dd x}
\end{equation}
What exactly does this mean? It emphatically does not meand that if you measure the position of one particle over and over, $\int x|\Psi1^2\dd x$ is the average of the results you'll get. Rather, $<x>$ is the average of measurements, performes on particles \textit{all in the satate} $\Psi$, which means that either you must find some way to returning the particle to its original state after each measurement, or else you have to prepare a whole \textbf{ensamble} of particles, each in the same state $\Psi$, and measure the positions of all of them: $<x>$ is the average of \textit{these} results. 

Now, as time goes on, $<x>$ will cahnge (because of the time dependence of $\Psi$), ando we might be interested in knowing how fast ir moves. We see that
\begin{equation}\label{1.31}
	\frac{\dd <x>}{\dd t}=-\frac{i\hbar}{m}\int\Psi^ *\pdv{\Psi}{x}\dd x
\end{equation}

The \textit{expectations value of the velocity is equal to the time derivative of the expectation value of position}:
\begin{equation}\label{1.32}
	<v>=\frac{\dd <x>}{\dd t}
\end{equation}

(\ref{1.31}) tell us, then, how to calculate $<v>$ directly from $\Psi$.

Acually, it is customary to work with \textbf{momentum} ($p=mv$), rather than velocity:
\begin{equation}\label{1.33}\marginnote{Momentum}
	<p>=m\frac{\dd <x>}{\dd t}=-i\hbar\int\left(\Psi^*\pdv{\Psi}{x}\right)\dd x
\end{equation}

Let me write the expressions for $<x>$ and $<p>$ in a more siggestive way:
\begin{equation}\label{1.34}
	<x>=\int\Psi^*(x)\Psi \dd x
\end{equation}
\begin{equation}\label{1.35}
	<p>=\int\Psi^*\left(\frac{\hbar}{i}\frac{\partial}{\partial x}\right)\Psi \dd x
\end{equation}

We say that the \textbf{operator} $x$ "represents" position, and the operator $(\hbar/i)(\partial/\partial x)$ "represents" momentum, in quantum mechanics; to calculate expectation values we "sandwich" the appropiate operator between $\Psi^*$ and $\Psi$, and integrate.

What about other quantities? The fact is, all classical dynamics variables can be expressed in terms of position and momentum.

To calculate the expectation value of any such quantuty, $Q(x,p)$, we simply replace every $p$ by $(\hbar/i)(\partial/\partial x)$, insert the resulting operator between $\Psi^*$ and $\Psi$, and integrate:
\begin{equation}\label{1.36}
	\boxed{<Q(x,p)>=\int\Psi^*Q\left(x,\frac{\hbar}{i}\frac{\partial}{\partial x}\right)\Psi\dd x}
\end{equation}

\section{The Uncertaintly Principle}
Teh wavelenght of $\Psi$ is related to the \textit{momentum} of the particle by the \textbf{de Broglie formula}:
\begin{equation}\label{1.39}\marginnote{deBroglie formula}
	p=\frac{\hbar}{\lambda}=\frac{2\pi\hbar}{\lambda}
\end{equation}
Thus a spread in \textit{wavelength} corresponds to a spread in \textit{momentum}, and our general observation now says that the more precisely determined a particle's position is, the ñedd precisely is its momentum. Quantitatively.
\begin{equation}\label{1.40}\marginnote{Uncertanty principle}
	\boxed{\sigma_x\sigma_p\geq\frac{\hbar}{2}}
\end{equation}
where $\sigma_x$ es the standard deviation in $x$, and $\sigma_p$ is the standard deviation in $p$.

Please understand what the uncertainty principle means: Like position measurements, momentum measurements yiled precise answers--the "spread" here refers to the fact that measurements on identically prepared systems do not yiled identical results. You can, if you want, construct a state such the repeated position measurements will be vey close together (bu making $\Psi$ a localized "spike"), but you sill pay a price: Momentum measurements on this state will be widely scattered. Oer you can prepare a state with a reproducible momentum (by making $\Psi$ a long sinusoidal wave), but in that case, position measurements will de widely scattered.
